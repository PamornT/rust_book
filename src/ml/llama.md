# LLaMA-rs

**LLaMA-rs** is a Rust port of the
[llama.cpp](https://github.com/ggerganov/llama.cpp) project. This allows running
inference for Facebook's [LLaMA](https://github.com/facebookresearch/llama)
model on a CPU with good performance using full precision, f16 or 4-bit
quantized versions of the model.

Just like its C++ counterpart, it is powered by the
[`ggml`](https://github.com/ggerganov/ggml) tensor library, achieving the same performance as the original code.

## Source

- https://github.com/setzer22/llama-rs

<details>
<summary>Conversions note</summary>

```mermaid
graph TD;
  A("PyTorch") --"<pre>1Ô∏è‚É£/2Ô∏è‚É£&nbsp;export_state_dict_checkpoint.py</pre>PyTorch model checkpoints (pth)"--> B(Python) --"<pre>3Ô∏è‚É£&nbsp;convert-pth-to-ggml.py</pre>Geometric Deep Learning Markup Language (ggml)"--> C(C++)--"<pre>4Ô∏è‚É£&nbsp;quantize.cpp</pre>Quantized ggml (bin)"-->D(Rust);
```

1Ô∏è‚É£ [tloen/alpaca-lora/export_state_dict_checkpoint.py](https://github.com/tloen/alpaca-lora/blob/main/export_state_dict_checkpoint.py) (llama-7b-hf)  
2Ô∏è‚É£ [jankais3r/LLaMA_MPS/export_state_dict_checkpoint.py](https://github.com/jankais3r/LLaMA_MPS/blob/main/export_state_dict_checkpoint.py) (llama-13b-hf)  
3Ô∏è‚É£ [llama.cpp/convert-pth-to-ggml.py](https://github.com/ggerganov/llama.cpp/blob/master/convert-pth-to-ggml.py)  
4Ô∏è‚É£ [llama.cpp/quantize.cpp](https://github.com/ggerganov/llama.cpp/blob/master/quantize.cpp)

</details>

---

## Overview

```mermaid
graph LR;
A --Apple Silicon GPU--> AA("üêç LLaMA_MPS")
A -..-> AAA("Raspberry Pi, Pixel 5, iPhone, NodeJS")
A("üêç llama") --"4-bit"--> B("üêá llama")
B --port w/ ggml--> C("ü¶Ä llama-rs")
A --"16,32-bit"--> CC("ü¶Ä RLLaMA")
C --"napi-rs"--> I("üê• llama-node")
E --"fine-tuning to obey ix"--> D("üêá alpaca")
A --instruction-following--> E("üêç alpaca") --LoRa--> F("üêç alpaca-lora")
E --instruction-following--> H("üêç codealpaca")
B --BLOOM-like--> BB("üêá bloomz")
BB --LoRA--> DDDD("üêç BLOOM-LoRA")
D --"fine-tunes the GPT-J 6B"--> DD("üêç Dolly")
D --"instruction-tuned Flan-T5"--> DDD("üêç Flan-Alpaca")
D --Alpaca_data_cleaned.json--> DDDD
E --RNN-->EE("üêç RWKV-LM")
H --finetuned--> EE
EE("üêç RWKV-LM") -..-> EEE("ü¶Ä smolrsrwkv")
A --"GPT-3.5-Turbo"--> FF("üêç gpt4all-lora")
A --"Apache0/nanoGPT"--> AAAA("üêç Lit-LLaMA")
```

- [üêç llama](https://github.com/facebookresearch/llama): Open and Efficient Foundation Language Models.
- [üêç LLaMA_MPS](https://github.com/jankais3r/LLaMA_MPS): Run LLaMA (and Stanford-Alpaca) inference on Apple Silicon GPUs.
- [üêá llama](https://github.com/ggerganov/llama.cpp): Inference of LLaMA model in pure C/C++.
- [üêá alpaca](https://github.com/antimatter15/alpaca.cpp): This combines the LLaMA foundation model with an open reproduction of Stanford Alpaca a fine-tuning of the base model to obey instructions (akin to the RLHF used to train ChatGPT) and a set of modifications to llama.cpp to add a chat interface.
- [ü¶Ä llama-rs](https://github.com/setzer22/llama-rs): Do the LLaMA thing, but now in Rust ü¶ÄüöÄü¶ô
- [üêç alpaca](https://github.com/tatsu-lab/stanford_alpaca): Stanford Alpaca: An Instruction-following LLaMA Model
- [üêç codealpaca](https://github.com/sahil280114/codealpaca): An Instruction-following LLaMA Model trained on code generation instructions.
- [üêç alpaca-lora](https://github.com/tloen/alpaca-lora): Low-Rank LLaMA Instruct-Tuning
- [üê• llama-node](https://github.com/hlhr202/llama-node): nodejs client library for llama LLM built on top of llama-rs. It uses napi-rs as nodejs and native communications.
- [ü¶Ä RLLaMA](https://github.com/Noeda/rllama): Rust+OpenCL+AVX2 implementation of LLaMA inference code.
- [üêç Dolly](https://github.com/databrickslabs/dolly): This fine-tunes the GPT-J 6B model on the Alpaca dataset using a Databricks notebook.
- [üêç Flan-Alpaca](https://github.com/declare-lab/flan-alpaca): Instruction Tuning from Humans and Machines.
- [üêá bloomz](https://github.com/NouamaneTazi/bloomz.cpp): Inference of HuggingFace's BLOOM-like models in pure C/C++ built on top of the amazing llama.cpp.
- [üêç BLOOM-LoRA](https://github.com/linhduongtuan/BLOOM-LORA): Low-Rank LLaMA Instruct-Tuning.
- [üêç RWKV-LM](https://github.com/BlinkDL/RWKV-LM): [ü§ó](https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B) RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, "infinite" ctx_len, and free sentence embedding.
- [ü¶Ä smolrsrwkv](https://github.com/KerfuffleV2/smolrsrwkv): A very basic example of the RWKV approach to language models written in Rust by someone that knows basically nothing about math or neural networks.
- [üêç gpt4all-lora](https://github.com/nomic-ai/gpt4all): A chatbot trained on a massive collection of clean assistant data including code, stories and dialogue.
- [üêç Lit-LLaMA](https://github.com/Lightning-AI/lit-llama): Independent implementation of LLaMA that is fully open source under the Apache 2.0 license. This implementation builds on nanoGPT.

## Alternatives

![](./assets/scaling-laws-blog-comparison.png)

```mermaid
graph LR;
Z("GPT")
Z-.Chinchilla formula..->Y("Cerebras-GPT")
Z-.Flamingo-style LMMs..-X("OpenFlamingo")
```

- [üêç Cerebras-GPT](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/): a family of seven GPT models ranging from 111 million to 13 billion parameters. Trained using the Chinchilla formula, these models provide the highest accuracy for a given compute budget. Cerebras-GPT has faster training times, lower training costs, and consumes less energy than any publicly available model to date.
- [üêç OpenFlamingo](https://laion.ai/blog/open-flamingo/): a framework that enables training and evaluation of large multimodal models (LMMs).

## Tools

```mermaid
graph TD;
AAA("üêç llama") --> BB("üêç LLaMA-Adapter")
AAA --> A
A("üêç langchain")
A --port--> AA("üê• langchainjs")
AA --> B("üê• langchain-alpaca")
D("üêá alpaca") --> B
E-..-D
E("üêá llama") --ggml/13B--> H
F("üêá whisper") --whisper-small--> H
H("üêá talk")
I("üêç chatgpt-retrieval-plugin") --> II("üêç llama-retrieval-plugin")
```

- [üêç langchain](https://github.com/hwchase17/langchain): Building applications with LLMs through composability.
- [üê• langchainjs](https://github.com/hwchase17/langchainjs): langchain in js.
- [üê• langchain-alpaca](https://github.com/linonetwo/langchain-alpaca): Run alpaca LLM fully locally in langchain.
- [üêá whisper](https://github.com/ggerganov/whisper.cpp): High-performance inference of OpenAI's Whisper automatic speech recognition (ASR) model.
- [ü§ó whisper-small](https://huggingface.co/openai/whisper-small): Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.
- [üêá talk](https://github.com/ggerganov/whisper.cpp/tree/master/examples/talk): Talk with an Artificial Intelligence in your terminal.
- [chatgpt-retrieval-plugin](https://github.com/openai/chatgpt-retrieval-plugin): The ChatGPT Retrieval Plugin lets you easily search and find personal or work documents by asking questions in everyday language.
- [llama-retrieval-plugin](https://github.com/lastmile-ai/llama-retrieval-plugin): LLaMa retrieval plugin script using OpenAI's retrieval plugin
- [üêç LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter): LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs .

## Hugging Face ü§ó

- [ü§ó Code Alpaca](https://huggingface.co/spaces/sahil2801/CodeAlpaca): `13B`, The Code Alpaca models are fine-tuned from a 7B and 13B LLaMA model on 20K instruction-following data generated by the techniques in the Self-Instruct [1] paper, with some modifications that we discuss in the next section. Evals are still a todo.
- [ü§ó Alpaca-LoRA-Serve](https://huggingface.co/spaces/chansung/Alpaca-LoRA-Serve): `7B`, Instruction fine-tuned version of LLaMA from Meta AI. Alpaca-LoRA is Low-Rank LLaMA Instruct-Tuning which is inspired by Stanford Alpaca project. This demo application currently runs 7B version on a T4 instance.

## ETC

> Refer to: https://replicate.com/blog/llama-roundup

- [Running LLaMA on a Raspberry Pi](https://twitter.com/miolini/status/1634982361757790209) by Artem Andreenko.
- [Running LLaMA on a Pixel 5](https://twitter.com/ggerganov/status/1635605532726681600) by Georgi Gerganov.
- [Run LLaMA and Alpaca with a one-liner](https://cocktailpeanut.github.io/dalai) ‚Äì `npx dalai llama`
- [Train and run Stanford Alpaca on your own machine](https://replicate.com/blog/replicate-alpaca) from replicate.
- [Fine-tune LLaMA to speak like Homer Simpson](https://replicate.com/blog/fine-tune-llama-to-speak-like-homer-simpson) from replicate.
- [Llamero](https://github.com/mpociot/llamero/) ‚Äì A GUI application to easily try out Facebook's LLaMA models by Marcel Pociot.
