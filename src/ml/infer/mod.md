# Inferences

- [x] [candle](./wasmedge.md) // MacOS/Windows/WSL2 // 27GB // 7.71 tokens/sec
- [x] [llamafile](https://future.mozilla.org/blog/introducing-llamafile/) // MacOS/Windows // 28GB (no gc) // 34.68 tokens/sec
- [x] [WasmEdge](./wasmedge.md) // MacOS/Windows/WSL2
- [x] [TabbyML](./tabbyml.md) // MacOS/Windows/WSL2
- [ ] [ollama](https://ollama.ai/) // MacOS
