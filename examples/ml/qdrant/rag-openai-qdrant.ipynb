{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be881a6d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Retrieval Augmented Generation (RAG) with OpenAI and Qdrant\n",
    "\n",
    "In the ever-evolving landscape of AI, the consistency and reliability of Large Language Models (LLMs) remain a challenge. While these models can understand statistical relationships between words, they often fail to provide accurate factual responses. Because their internal knowledge may not be accurate, outputs can range from spot-on to nonsensical. Retrieval Augmented Generation (RAG) is a framework designed to bolster the accuracy of LLMs by grounding them in external knowledge bases. In this example, we'll demonstrate a streamlined  implementation of the RAG pipeline using only Qdrant and OpenAI SDKs. By harnessing Flag embedding's power, we can bypass additional frameworks' overhead. \n",
    "    \n",
    "This example assumes you understand the architecture necessary to carry out RAG. If this is new to you, please look at some introductory readings:\n",
    "* [Retrieval-Augmented Generation: To add knowledge](https://eugeneyan.com/writing/llm-patterns/#retrieval-augmented-generation-to-add-knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb044259",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Let's start setting up all the pieces to implement the RAG pipeline. We will only use Qdrant and OpenAI SDKs, without any third-party libraries. \n",
    "\n",
    "### Preparing the environment\n",
    "\n",
    "We need just a few dependencies to implement the whole application, so let's start with installing the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce9f81b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T10:06:32.977456Z",
     "start_time": "2023-09-27T10:06:30.203757Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting qdrant-client\n",
      "  Downloading qdrant_client-1.7.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting fastembed\n",
      "  Downloading fastembed-0.1.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting grpcio>=1.41.0 (from qdrant-client)\n",
      "  Downloading grpcio-1.60.0-cp39-cp39-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-tools>=1.41.0 (from qdrant-client)\n",
      "  Downloading grpcio_tools-1.60.0-cp39-cp39-macosx_10_10_universal2.whl.metadata (6.2 kB)\n",
      "Collecting httpx>=0.14.0 (from httpx[http2]>=0.14.0->qdrant-client)\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting numpy>=1.21 (from qdrant-client)\n",
      "  Using cached numpy-1.26.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting pydantic>=1.10.8 (from qdrant-client)\n",
      "  Downloading pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<2.0.0,>=1.26.14 (from qdrant-client)\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub==0.19.4 (from fastembed)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting onnx<2.0,>=1.11 (from fastembed)\n",
      "  Downloading onnx-1.15.0-cp39-cp39-macosx_10_12_universal2.whl.metadata (15 kB)\n",
      "Collecting onnxruntime<2.0,>=1.15 (from fastembed)\n",
      "  Downloading onnxruntime-1.16.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.3 kB)\n",
      "Collecting requests<3.0,>=2.31 (from fastembed)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.16.0,>=0.15.0 (from fastembed)\n",
      "  Downloading tokenizers-0.15.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm<5.0,>=4.65 (from fastembed)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from huggingface-hub==0.19.4->fastembed)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub==0.19.4->fastembed)\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub==0.19.4->fastembed)\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from huggingface-hub==0.19.4->fastembed) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from huggingface-hub==0.19.4->fastembed) (23.2)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Collecting protobuf<5.0dev,>=4.21.6 (from grpcio-tools>=1.41.0->qdrant-client)\n",
      "  Downloading protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (58.0.4)\n",
      "Collecting certifi (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client)\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h2<5,>=3 (from httpx[http2]>=0.14.0->qdrant-client)\n",
      "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting coloredlogs (from onnxruntime<2.0,>=1.15->fastembed)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers (from onnxruntime<2.0,>=1.15->fastembed)\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting sympy (from onnxruntime<2.0,>=1.15->fastembed)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic>=1.10.8->qdrant-client)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.5 (from pydantic>=1.10.8->qdrant-client)\n",
      "  Downloading pydantic_core-2.14.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0,>=2.31->fastembed)\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client)\n",
      "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client)\n",
      "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2.0,>=1.15->fastembed)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpmath>=0.19 (from sympy->onnxruntime<2.0,>=1.15->fastembed)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading qdrant_client-1.7.0-py3-none-any.whl (203 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastembed-0.1.3-py3-none-any.whl (14 kB)\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.6.0-py3-none-any.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.60.0-cp39-cp39-macosx_10_10_universal2.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_tools-1.60.0-cp39-cp39-macosx_10_10_universal2.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-1.26.2-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Downloading onnx-1.15.0-cp39-cp39-macosx_10_12_universal2.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.16.3-cp39-cp39-macosx_11_0_arm64.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.14.5-cp39-cp39-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp39-cp39-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m763.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp39-cp39-macosx_11_0_arm64.whl (120 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.4/120.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp39-cp39-macosx_11_0_arm64.whl (174 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.4/174.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Installing collected packages: mpmath, flatbuffers, urllib3, tqdm, sympy, sniffio, pyyaml, pydantic-core, protobuf, portalocker, numpy, idna, hyperframe, humanfriendly, hpack, h11, grpcio, fsspec, filelock, distro, charset-normalizer, certifi, annotated-types, requests, pydantic, onnx, httpcore, h2, grpcio-tools, coloredlogs, anyio, onnxruntime, huggingface-hub, httpx, tokenizers, openai, qdrant-client, fastembed\n",
      "Successfully installed annotated-types-0.6.0 anyio-4.2.0 certifi-2023.11.17 charset-normalizer-3.3.2 coloredlogs-15.0.1 distro-1.8.0 fastembed-0.1.3 filelock-3.13.1 flatbuffers-23.5.26 fsspec-2023.12.2 grpcio-1.60.0 grpcio-tools-1.60.0 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.2 httpx-0.26.0 huggingface-hub-0.19.4 humanfriendly-10.0 hyperframe-6.0.1 idna-3.6 mpmath-1.3.0 numpy-1.26.2 onnx-1.15.0 onnxruntime-1.16.3 openai-1.6.0 portalocker-2.8.2 protobuf-4.25.1 pydantic-2.5.2 pydantic-core-2.14.5 pyyaml-6.0.1 qdrant-client-1.7.0 requests-2.31.0 sniffio-1.3.0 sympy-1.12 tokenizers-0.15.0 tqdm-4.66.1 urllib3-1.26.18\n"
     ]
    }
   ],
   "source": [
    "!pip install qdrant-client fastembed openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4382a",
   "metadata": {},
   "source": [
    "[Qdrant](https://qdrant.tech) will act as a knowledge base providing the context information for the prompts we'll be sending to the LLM. There are various ways of running Qdrant, but we'll simply use the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f4456c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T10:06:34.283299Z",
     "start_time": "2023-09-27T10:06:32.980517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find image 'qdrant/qdrant:latest' locally\n",
      "latest: Pulling from qdrant/qdrant\n",
      "\n",
      "\u001b[1B21e92a36: Pulling fs layer \n",
      "\u001b[1Ba158e66e: Pulling fs layer \n",
      "\u001b[1B4b9a0162: Pulling fs layer \n",
      "\u001b[1Bae58d069: Pulling fs layer \n",
      "\u001b[1Bfc44e8ca: Pulling fs layer \n",
      "\u001b[1B5b21cf92: Pulling fs layer \n",
      "\u001b[1Bbb8b14b1: Pulling fs layer \n",
      "\u001b[1Bb700ef54: Pull complete   32B/32B61MBB\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2KDigest: sha256:5bd665cb6e9ba3ecb3bc5a851226a6ca0b70b6165574482132edeb606f21d1a5\n",
      "Status: Downloaded newer image for qdrant/qdrant:latest\n",
      "360e4526743326b936a542409c0325b407d211784e5d150bc7a4b6fb2dabf4f5\n"
     ]
    }
   ],
   "source": [
    "!docker run -p \"6333:6333\" -p \"6334:6334\" --name \"rag-openai-qdrant\" --rm -d qdrant/qdrant:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c7a21",
   "metadata": {},
   "source": [
    "### Creating the collection\n",
    "\n",
    "Qdrant [collection](https://qdrant.tech/documentation/concepts/collections/) is the basic unit of organizing your data. Each collection is a named set of points (vectors with a payload) among which you can search. After connecting to our running Qdrant container, we can check whether we already have some collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd8966b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T10:06:36.242783Z",
     "start_time": "2023-09-27T10:06:34.289290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import qdrant_client\n",
    "\n",
    "client = qdrant_client.QdrantClient(\"http://localhost:6333\", prefer_grpc=True)\n",
    "client.get_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f54205",
   "metadata": {},
   "source": [
    "### Building the knowledge base\n",
    "\n",
    "Qdrant will use vector embeddings of our facts to enrich the original prompt with some context. Thus, we need to store the vector embeddings and the texts used to generate them. All our facts will have a JSON payload with a single attribute and look as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"document\": \"Binary Quantization is a method of reducing the memory usage even up to 40 times!\"\n",
    "}\n",
    "```\n",
    "\n",
    "This structure is required by [FastEmbed](https://qdrant.github.io/fastembed/), a library that simplifies managing the vectors, as you don't have to calculate them on your own. It's also possible to use an existing collection, However, all the code snippets will assume this data structure. Adjust your examples to work with a different schema.\n",
    "\n",
    "FastEmbed will automatically create the collection if it doesn't exist. Knowing that we are set to add our documents to a collection, which we'll call `knowledge-base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43154775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T10:06:36.692231Z",
     "start_time": "2023-09-27T10:06:36.245915Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77.7M/77.7M [00:06<00:00, 12.1MiB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['f07ac6377d9645a08f353f22e70720fa',\n",
       " 'd3ba66b4f1dc42d2b5d02895b93c51dd',\n",
       " '75bdd14f85b4464f8fc61b223f56a422',\n",
       " 'cda6a062fe054ac09ba9acf6e854ac1e',\n",
       " '78f916552e0b428fa3a619a374522f6a',\n",
       " '3eb4bb75fafe423fbea1de96114c78dc',\n",
       " '31b8d858413a4b908f63be128f23fa9b',\n",
       " 'dcc01a68ff314ad1a42aa138912f881c']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.add(\n",
    "    collection_name=\"knowledge-base\",\n",
    "    documents=[\n",
    "        \"Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\",\n",
    "        \"Docker helps developers build, share, and run applications anywhere — without tedious environment configuration or management.\",\n",
    "        \"PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.\",\n",
    "        \"MySQL is an open-source relational database management system (RDBMS). A relational database organizes data into one or more data tables in which data may be related to each other; these relations help structure the data. SQL is a language that programmers use to create, modify and extract data from the relational database, as well as control user access to the database.\",\n",
    "        \"NGINX is a free, open-source, high-performance HTTP server and reverse proxy, as well as an IMAP/POP3 proxy server. NGINX is known for its high performance, stability, rich feature set, simple configuration, and low resource consumption.\",\n",
    "        \"FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\",\n",
    "        \"SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.\",\n",
    "        \"The cron command-line utility is a job scheduler on Unix-like operating systems. Users who set up and maintain software environments use cron to schedule jobs (commands or shell scripts), also known as cron jobs, to run periodically at fixed times, dates, or intervals.\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36bddd6",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation\n",
    "\n",
    "RAG changes the way we interact with Large Language Models. We're converting a knowledge-oriented task, in which the model may create a counterfactual answer, into a language-oriented task. The latter expects the model to extract meaningful information and generate an answer. LLMs, when implemented correctly, are supposed to be carrying out language-oriented tasks.\n",
    "\n",
    "The task starts with the original prompt sent by the user. The same prompt is then vectorized and used as a search query for the most relevant facts. Those facts are combined with the original prompt to build a longer prompt containing more information.\n",
    "\n",
    "But let's start simply by asking our question directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed31ca63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T10:06:36.695165Z",
     "start_time": "2023-09-27T10:06:36.695150Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "What tools should I need to use to build a web service using vector embeddings for search?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d2d7dd",
   "metadata": {},
   "source": [
    "Using OpenAI API requires providing the API key. Our example demonstrates setting the `OPENAI_API_KEY` using an environmental variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e8669e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T10:06:36.696985Z",
     "start_time": "2023-09-27T10:06:36.696959Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Fill the environmental variable with your own OpenAI API key\n",
    "# See: https://platform.openai.com/account/api-keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<< PASS YOUR OWN KEY >>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf5c684",
   "metadata": {},
   "source": [
    "Now we can finally call the completion service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5cdee82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T10:06:36.700541Z",
     "start_time": "2023-09-27T10:06:36.700518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Choice(finish_reason='stop', index=None, logprobs=None, message=ChatCompletionMessage(content='To build a web service using vector embeddings for search, you will need a combination of tools and technologies. Here are some essential ones:\\n\\n1. Programming Language: You\\'ll need a programming language to develop the backend of your web service. Some popular options for this task are Python, Java, or Node.js.\\n\\n2. Web Framework: A web framework will help you manage routing, handle HTTP requests, and simplify the development process. Examples include Django (Python), Flask (Python), Spring Boot (Java), or Express (Node.js).\\n\\n3. Vector Embeddings Library: You\\'ll need a library to create and manipulate vector embeddings. Some widely used libraries are TensorFlow, PyTorch, or gensim. These libraries provide pre-trained models and tools to build your own embeddings.\\n\\n4. Search Engine: To perform efficient search operations on the vector embeddings, you\\'ll require a search engine. Elasticsearch is a popular choice for its scalability, full-text search capabilities, and support for vector similarity searches through its \"dense_vector\" type (recent versions).\\n\\n5. API Development: To expose your web service\\'s functionality, you\\'ll need to develop an API. Tools like Django REST Framework or Flask-RESTful (Python) can assist you in building RESTful APIs.\\n\\n6. Containerization: It\\'s recommended to containerize your web service for easy deployment and scalability. Docker is a popular tool for containerization, allowing you to package your application with its dependencies and run it in any environment consistently.\\n\\n7. Web Server: You\\'ll need a web server to handle HTTP requests and route them to your web service. Popular choices include Apache or Nginx, which can be configured to work with your chosen web framework.\\n\\n8. Infrastructure & Deployment: Depending on the scale and requirements of your web service, you may need infrastructure to host and deploy your application. Platforms like AWS, Google Cloud, or Microsoft Azure can provide the necessary infrastructure and deployment tools.\\n\\nRemember, the choice of specific tools may vary based on your familiarity with them, project requirements, and personal preference.', role='assistant', function_call=None, tool_calls=None))]\n"
     ]
    }
   ],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "openai = AsyncOpenAI()\n",
    "openai.base_url = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "completion = await openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b420d81d",
   "metadata": {},
   "source": [
    "### Extending the prompt\n",
    "\n",
    "Even though the original answer sounds credible, it didn't answer our question correctly. Instead, it gave us a generic description of an application stack. To improve the results, enriching the original prompt with the descriptions of the tools available seems like one of the possibilities. Let's use a semantic knowledge base to augment the prompt with the descriptions of different technologies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce791ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T10:06:36.702641Z",
     "start_time": "2023-09-27T10:06:36.702619Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[QueryResponse(id='f07ac637-7d96-45a0-8f35-3f22e70720fa', embedding=None, metadata={'document': 'Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!'}, document='Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!', score=0.8717050552368164),\n",
       " QueryResponse(id='3eb4bb75-fafe-423f-bea1-de96114c78dc', embedding=None, metadata={'document': 'FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.'}, document='FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.', score=0.845363438129425),\n",
       " QueryResponse(id='75bdd14f-85b4-464f-8fc6-1b223f56a422', embedding=None, metadata={'document': 'PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.'}, document='PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.', score=0.8426207304000854)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = client.query(\n",
    "    collection_name=\"knowledge-base\",\n",
    "    query_text=prompt,\n",
    "    limit=3,\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6640067",
   "metadata": {},
   "source": [
    "We used the original prompt to perform a semantic search over the set of tool descriptions. Now we can use these descriptions to augment the prompt and create more context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a16d8549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\\nFastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\\nPyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"\\n\".join(r.document for r in results)\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c04a4e",
   "metadata": {},
   "source": [
    "Finally, let's build a metaprompt, the combination of the assumed role of the LLM, the original question, and the results from our semantic search that will force our LLM to use the provided context. \n",
    "\n",
    "By doing this, we effectively convert the knowledge-oriented task into a language task and hopefully reduce the chances of hallucinations. It also should make the response sound more relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fc9a98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a software architect. \n",
      "Answer the following question using the provided context. \n",
      "If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
      "\n",
      "Question: What tools should I need to use to build a web service using vector embeddings for search?\n",
      "\n",
      "Context: \n",
      "Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\n",
      "FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\n",
      "PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metaprompt = f\"\"\"\n",
    "You are a software architect. \n",
    "Answer the following question using the provided context. \n",
    "If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
    "\n",
    "Question: {prompt.strip()}\n",
    "\n",
    "Context: \n",
    "{context.strip()}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Look at the full metaprompt\n",
    "print(metaprompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a7678",
   "metadata": {},
   "source": [
    "Our current prompt is much longer, and we also used a couple of strategies to make the responses even better:\n",
    "\n",
    "1. The LLM has the role of software architect.\n",
    "2. We provide more context to answer the question.\n",
    "3. If the context contains no meaningful information, the model shouldn't make up an answer.\n",
    "\n",
    "Let's find out if that works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "709b9f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To build a web service using vector embeddings for search, you can use the following tools:\n",
      "\n",
      "1. Qdrant: Qdrant is a vector database and vector similarity search engine that can be deployed as an API service. It provides search for nearest high-dimensional vectors, making it suitable for vector embeddings and search applications.\n",
      "\n",
      "2. FastAPI: FastAPI is a modern, fast, and high-performance web framework for building APIs with Python 3.7+. It is based on standard Python type hints, making it easy to integrate with other Python libraries and frameworks.\n",
      "\n",
      "3. PyTorch: PyTorch is a machine learning framework that can be used for applications such as computer vision and natural language processing. It provides support for neural networks and can be used to implement vector embeddings and other machine learning algorithms.\n",
      "\n",
      "These tools can be used together to build a web service that incorporates vector embeddings for search. Qdrant can be used as the vector database and search engine, while FastAPI and PyTorch can be used for building the web service and implementing the machine learning algorithms, respectively.\n"
     ]
    }
   ],
   "source": [
    "completion = await openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": metaprompt},\n",
    "    ],\n",
    "    timeout=10.0,\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4120e1-9899-4caa-b974-51d9b3a485be",
   "metadata": {},
   "source": [
    "### Testing out the RAG pipeline\n",
    "\n",
    "By leveraging the semantic context we provided our model is doing a better job answering the question. Let's enclose the RAG as a function, so we can call it more easily for different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62ed09d1-2c90-4ffc-9f1d-7beb87bab78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rag(question: str, n_points: int = 3) -> str:\n",
    "    results = client.query(\n",
    "        collection_name=\"knowledge-base\",\n",
    "        query_text=question,\n",
    "        limit=n_points,\n",
    "    )\n",
    "\n",
    "    context = \"\\n\".join(r.document for r in results)\n",
    "\n",
    "    metaprompt = f\"\"\"\n",
    "    You are a software architect. \n",
    "    Answer the following question using the provided context. \n",
    "    If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
    "    \n",
    "    Question: {question.strip()}\n",
    "    \n",
    "    Context: \n",
    "    {context.strip()}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    completion = await openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": metaprompt},\n",
    "        ],\n",
    "        timeout=10.0,\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fecd76-9a0b-4ad1-9097-b1d292a618ac",
   "metadata": {},
   "source": [
    "Now it's easier to ask a broad range of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa0fdead-a115-4fcd-88dc-5cc718dc0544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The stack for a web API built with FastAPI can include FastAPI itself as the web framework, Docker for containerization and deployment, and NGINX as a reverse proxy for load balancing and SSL termination.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await rag(\"What can the stack for a web api look like?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7324c127-c140-410a-ab19-87a5babce023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await rag(\"Where is the nearest grocery store?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe56730-ed41-42c1-9c33-de3849c60b65",
   "metadata": {},
   "source": [
    "Our model can now:\n",
    "\n",
    "1. Take advantage of the knowledge in our vector datastore.\n",
    "2. Answer, based on the provided context, that it can not provide an answer.\n",
    "\n",
    "We have just shown a useful mechanism to mitigate the risks of hallucinations in Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a3ae6",
   "metadata": {},
   "source": [
    "### Cleaning up the environment\n",
    "\n",
    "If you wish to continue playing with the RAG application we created, don't do the code below. However, it's always good to clean up the environment, so nothing is left dangling. We'll show you how to remove the Qdrant container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0729043",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T10:06:36.704761Z",
     "start_time": "2023-09-27T10:06:36.704742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag-openai-qdrant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response from daemon: No such container: rag-openai-qdrant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!docker kill rag-openai-qdrant\n",
    "!docker rm rag-openai-qdrant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
